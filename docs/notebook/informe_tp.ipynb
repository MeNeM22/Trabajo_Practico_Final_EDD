{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aa6c5a0",
   "metadata": {},
   "source": [
    "## Descripcion del problema \n",
    "El trabajo consiste en desarrollar un sistema en Python capaz de recuperar, procesar y organizar información proveniente de \n",
    "distintas fuentes de la Web, aplicando varias técnicas de obtención de datos: \n",
    "consumo de APIs, web scraping y lectura de feeds RSS.\n",
    "\n",
    "El programa debe obtener información de tres tipos de fuentes diferentes:\n",
    "\n",
    "APIs: OpenAlex y The Lens, para consultar artículos científicos y patentes.\n",
    "\n",
    "Sitios web: Eventseye, Nferias y 10Times, para extraer datos de eventos y ferias mediante scraping.\n",
    "\n",
    "Feeds RSS: fuentes relacionadas a comercio internacional (WTO y Comtrade Plus) para obtener noticias recientes.\n",
    "\n",
    "Cada conjunto de datos debe ser procesado y almacenado en archivos CSV, asegurando consistencia en los metadatos extraídos. Además, el sistema debe \n",
    "incluir una interfaz por consola que permita ejecutar las consultas y visualizar los datos generados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6717a1",
   "metadata": {},
   "source": [
    "## Análisis y Diseño\n",
    "\n",
    "### Parte 1: APIs\n",
    "El diseño implementado esta creado con distintas funciones para dividir las responsabilidades.\n",
    "### Metodo para obtener los datos de la API Open Alex:\n",
    "    -Funcion: buscar_articulos_openalex()\n",
    "    -Proposito: Este se encarga de obtener los articulos mas recientes, siendo el unico metodo de contacto con la API.\n",
    "    -Diseño: \n",
    "        -Se utilizo la biblioteca \"requests\" para realizar las solicitudes HTTP.\n",
    "        -En la cabecera del metodo se ingresan los paramaetros para realizar la consulta (por-page y search), permitiendo que la busqueda sea flexible.\n",
    "        -Incluye manejo de errores.\n",
    "\n",
    "### Metodos para procesamiento y extracion de Metadatos:\n",
    "    -Funciones: extraer_datos_de_autores_intitucion(), extraer_palabras_clave(), extraer_pais_de_publicacion(), extraer_campos_de_estudio() y extraer_resumen().\n",
    "    -Proposito: Este conjunto de funciones permite la division de distintas responsabilidades, esto facilita la busqueda de errores, en caso de que este fallando un caso en especial.\n",
    "    -Diseño:\n",
    "        -Cada metodo se encarga de extraer un tipo de metadato.\n",
    "        -Inclute Manejo de errores.\n",
    "\n",
    "### Metodo de transformacion:\n",
    "    -Funcion: extraer_metadatos()\n",
    "    -Proposito: Se encarga de convertir la lista de articulos obtenida, en una lista de diccionarios para luego ser guardada en un archivo CSV.\n",
    "    -Diseño:\n",
    "        -Itera sobre cada articulo encontrado, en cada uno de estos extrae los metadatos utilizando los metodos creados, limpia los datos procesados, luego los ensambla en un diccionario y este lo agrega a una lista.\n",
    "\n",
    "### Metodo de persistencia de datos:\n",
    "    -Funcion: guardar_en_csv()\n",
    "    -Proposito: Toma los datos limpios de cada articulo almacenado y los guarda en ..data/articulos.CSV\n",
    "    -Diseño:\n",
    "        -Se utilizo la biblioteca csv, nos brinda los metodos utiles como DictWriter permitiendonos escribir las cabeceras y las filas.\n",
    "\n",
    "### Complejidad de metodos principales:\n",
    "    -buscar_articulos_openalex(): Su complejidad es de O(n) ya que n seria la cantidad de articulos que OpenAlex devuelve en results, la complejidad estaria ligada al tamaño de la respuesta.\n",
    "    -extraer_metadatos(): Su complejidad es de O(n) al tener un for que va a recorrer segun la cantidad de articulos obtenidos.\n",
    "    -guardar_en_csv(): Su complejidad es de O(n) al escribir  n filas en el archivo CSV, cada fila implica una operacion.\n",
    "    -mostrar_datos_csv_openalex(): Su complejidad es de O(n) al leer n filas del archivo CSV y las imprime.\n",
    "    -consultar_openalex(): Su complejidad es de O(n) al ejecutar las siguientes funciones, buscar_articulos_openalex(), extraer_metadatos(), guardar_en_csv()\n",
    "\n",
    "### Estructura de datos aplicadas:\n",
    "    -Diccionarios:\n",
    "        -Permite representar objetos con propiedas.\n",
    "        -El metodo extraer_metadatos() convierte los articulos a dicconarios, esto permite un acceso rapido a cada metadato del articulo.\n",
    "        -El metodo buscar_articulos_openalex() almacena los paramaetros de consulta en un diccionario, ya que la biblioteca requests espera los parametros en la URL.\n",
    "    -Listas:\n",
    "        -Contenedores que representa colecciones ordenadas\n",
    "        -El metodo extraer_metadatos() utiliza un contenedor que es una lista de diccionarios.\n",
    "    -Conjuntos:\n",
    "        -Este no permite datos duplicados, garantiza la unicidad de datos.\n",
    "        -El metodo extraer_pais_de_publicacion utiliza set ya que un articulo puede tener varios autores del mismo pais o institucion, evitando duplicarlos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9ceb18",
   "metadata": {},
   "source": [
    "### Parte 2: Web Scraping\n",
    "\n",
    "### Librerias utilizadas:\n",
    "    -RobotFileParser: Usado para poder realizar las consultas de robots.txt y poder respetar las paginas a scrapear\n",
    "    -request y BeautifulSoup: La base del scrap\n",
    "    -urljoin: Utilizado para poner unir URL's más fácilmente\n",
    "    -re: Usado para obtener el correo en determinadas páginas, ya que cambiaba de posición constantemente\n",
    "    -os y csv: Utilizado para poder crear y modificar los archivos .csv\n",
    "    -cloudscraper: Utilizado exclusivamente para acceder 10Times, ya que usando requests denegaba el acceso.\n",
    "\n",
    "\n",
    "### - Análisis de los sitios web objetivo\n",
    "El proceso de scraping en nferias y eventseye fueron parecidos. Ambos tienen una estructura simple y parecida. Nferias requiere un poco más técnica por su forma de organizar la información. Ambos poseian una estructura HTML similar, intuitiva y lógica.\n",
    "En cambio, 10times si requirió una técnica mucho más avanzada, debido a la utilización de JavaScript para cargar contenido dinámico. Además de tener una estructura HTML insual y confusa.\n",
    "\n",
    " Complejidad espacial y temporal:\n",
    " \n",
    "    NFERIAS: \n",
    "    Complejidad temporal: O(i. k. j) siendo I: limiteIndustrias, k: ferias_por_industria y j: la cantidad de paginas que se vayan a requerir para llegar a k ferias\n",
    "\n",
    "    Complejidad espacial: O(1)\n",
    "\n",
    "    EVENTSEYE:\n",
    "        Complejidad temporal: O(L · S · E) siendo L: letras, S: sectores y E: eventos\n",
    "\n",
    "        Complejidad espacial: O(1)\n",
    "\n",
    "    10TIMES:\n",
    "        Complejidad temporal: O(C · E) siendo C: categorias y E: eventos\n",
    "\n",
    "        Complejidad espacial: O(1)\n",
    "\n",
    "### - Diseño del scraper\n",
    " El diseño del scraper de nferias y eventseye son similares, parten de una URL semillas, realiza las solicitudes para poder seguir extrayendo URL's hasta llegar a la URL donde extraerá la informacion que nos interesa.\n",
    " \n",
    " El diseño de 10times no es muy diferente a los demas. Parte de una url semilla en la que recorre las categorias y  en cada categoria los eventos, por cada evento visitado se extrae su informacion. Cuando los eventos alcanzan el limite preestablecido el scrap, este entra en una nueva categoria. Cuando esta alcanza el limite termina la ejecucion. Este algoritmo no cuenta con paginacion, no extrae la web del evento ni el contacto, ya que la pagina carga sus paginas dinamicamente con JavaScript y usa funciones del mismo lenguaje para acceder a los mencionados datos, problema que no hemos podido resolver.\n",
    " Otro problema con el que cuenta este algoritmo, es que no puede usarse tan seguido. Una vez usado se debe esperar unos minutos para volverse a usar, ya que al querer hacerlo empieza a tener errores que antes no tenia.\n",
    "\n",
    "### - Estrategias de extracción de datos\n",
    "    El objetivo principal de extraccion a cada página consistia en llegar a cada evento/feria individual en la menor cantidad de request posibles. Por lo que optamos por empezar las request desde una pagina que ya contenga todas las industrias/sectores de la pagina. Una vez llegado al evento la extraccion de toda la informacion necesario, la extracción pasa un proceso antes de guardarse:\n",
    "    Se tokeniza, se saca espacios extras y une todo con solamente un espacio, si es necesario. Si la información es una estructura de datos que no sea una cadena de texto, se la transforma en una. En caso de tener texto o signos de puntuacion no deseados, se utiliza regex para extraerlos.\n",
    "\n",
    "\n",
    "### - Estructuras de datos aplicadas\n",
    "Además de listas, se utilizaron diccionarios para almanenar los datos internamente y la generación de un archivo .csv para la lectura del usuario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a942c0c8",
   "metadata": {},
   "source": [
    "#### Parte 3: Analisis RSS\n",
    "##### 1. Introducción\n",
    "\n",
    "    Se desarrolló un módulo encargado de consumir, procesar y almacenar información proveniente de feeds RSS utilizando la librería feedparser. El sistema permite consultar  BBC o El País, extraer datos relevantes de cada noticia, identificar países mencionados y registrar la información de manera persistente evitando duplicados.\n",
    "\n",
    "    El procesamiento principal se encuentra implementado en el archivo:\n",
    "    /src/rss/lector.py\n",
    "\n",
    "##### 2. Arquitectura General del Parser\n",
    "\n",
    "    El parser fue diseñado con tres objetivos principales:\n",
    "\n",
    "    Consumir varios feeds RSS de forma dinámica según la selección del usuario.\n",
    "\n",
    "    Normalizar y estructurar la información en un formato interno uniforme.\n",
    "\n",
    "    Persistir los datos sin duplicar noticias, aun si el usuario ejecuta el lector varias veces.\n",
    "\n",
    "    La función principal del módulo es leer_rss(url, nombre_pagina).\n",
    "\n",
    "##### 3. Lectura de RSS con feedparser\n",
    "\n",
    "    Se empleó la librería feedparser, que permite analizar documentos RSS mediante el método:\n",
    "\n",
    "    feed = feedparser.parse(url)\n",
    "\n",
    "\n",
    "    Cada item del feed se representa como entry y contiene atributos tales como:(cada entry se obtienen de feed.entries al recorrer con for)\n",
    "\n",
    "    entry.title\n",
    "\n",
    "    entry.summary\n",
    "\n",
    "    entry.published\n",
    "\n",
    "    entry.tags (si existen)\n",
    "\n",
    "    A partir de estos datos se construye un diccionario por noticia con la información relevante.\n",
    "\n",
    "##### 4. Extracción de Información Relevante\n",
    "4.1 Campos almacenados por noticia\n",
    "\n",
    "    Para cada entrada del RSS se extraen y procesan los siguientes datos:\n",
    "\n",
    "    Título\n",
    "\n",
    "    Descripción\n",
    "\n",
    "    Fecha de publicación (con traducción del día al español)\n",
    "\n",
    "    Países relacionados\n",
    "\n",
    "    ID única generada por hash\n",
    "\n",
    "##### 5. Complejidad temporal y espacial de cada funcion\n",
    "\n",
    "### Funcion: obtener_dia_semana(fecha_str):\n",
    "Se realiza un split, y operaciones sobre cada string:\n",
    "Temporal y espacial: O(n)\n",
    "\n",
    "### Funcion: buscar_pais_en_data(arreglo, titulo, descripcion)\n",
    "Temporal:\n",
    "    Recorre:\n",
    "        1.Set paises: O(n)\n",
    "        2.Tags: O(n)\n",
    "        3. Buscar titulo y desc: O(n)\n",
    "Espacial:\n",
    "    Set con n paises: O(n)\n",
    "\n",
    "### Funcion: obtener_id_visitadas()\n",
    "Temporal:\n",
    "    Recorre:\n",
    "        1. Archivo en memoria: O(n)\n",
    "Espacial:\n",
    "    Set con n lineas: O(n)\n",
    "\n",
    "\n",
    "### Funcion guardar_id_visitada(id_noticia): \n",
    "Temporal y espacial:\n",
    "    1. Escribe sobre un archivo: O(1)\n",
    "\n",
    "### Funcion leer_rss(url,nombre_pagina):\n",
    "Temporal:\n",
    "    1. Recorre feed entries: O(n)\n",
    "\n",
    "Espacial: \n",
    "    1.Las guarda en una lista con n elementos: O(n)\n",
    "\n",
    "###  guardar_csv(noticias, nombre_pagina):\n",
    "Temporal:\n",
    "    1. Recorre nuevos registros y existentes: O(n+m) = O(n)\n",
    "Espacial:\n",
    "    1. Crea una lista combinada de longitud total n\n",
    "\n",
    "### Visualizar csv rss:\n",
    "Temporal.\n",
    "    1. Imprimir y leer cada fila: O(n)\n",
    "Espacial: O(1)\n",
    "\n",
    "### Mostrar noticias:\n",
    "Temporal:\n",
    "    1. Recorre la lista de noticias: O(n)\n",
    "Espacial: O(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72edc092",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
